{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cc9b03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risko\\miniconda3\\envs\\general\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dataloader import get_eval_datasets\n",
    "from widemlp import MLP, prepare_inputs_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24582140",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda:0\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "MODEL_NAME = \"\"\n",
    "EPOCHS = 1\n",
    "NUM_CLASSES = 3\n",
    "THRESHOLD = 0.5\n",
    "DATASET_SIZE = 15_000\n",
    "TEST_SPLIT = 0.2\n",
    "NUM_HIDDEN_LAYERS = 3\n",
    "NUM_CLASSES = 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09048473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a79f8a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = get_eval_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ef20f",
   "metadata": {},
   "source": [
    "# MLP 3cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa13e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = torch.load(\"widemlp_idf.pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\n",
    "    \"widemlp.pt\", weights_only=True, map_location=torch.device(DEVICE)\n",
    ")\n",
    "wide_mlp = MLP(\n",
    "    vocab_size=len(tokenizer),\n",
    "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    idf=idf,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "wide_mlp.to(DEVICE)\n",
    "wide_mlp.idf = idf if wide_mlp.idf is not None else None\n",
    "wide_mlp.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "wide_mlp.eval()\n",
    "print(f\"Successfully loaded PyTorch model on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baedbcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(\n",
    "    model: MLP,\n",
    "    dataset_df: pd.DataFrame,\n",
    "    threshold: float = THRESHOLD,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    ") -> tuple:\n",
    "    dataset = Dataset.from_pandas(dataset_df)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            inputs = tokenizer(\n",
    "                batch[\"prompt\"],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(DEVICE)\n",
    "            flat_inputs, offsets = prepare_inputs_optimized(\n",
    "                inputs[\"input_ids\"], device=DEVICE\n",
    "            )\n",
    "            logits = model(flat_inputs, offsets)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            for probs in probabilities:\n",
    "                thresholded_indices = torch.where(probs > threshold)[0]\n",
    "                if len(thresholded_indices) == 0:\n",
    "                    # This means that all probabilities are below the threshold == model is not confident to pick any class\n",
    "                    pred.append(0)\n",
    "                else:\n",
    "                    # This means that at least one class is above the threshold\n",
    "                    pred.append(1)\n",
    "    return pred, dataset_df[\"label\"].tolist()\n",
    "\n",
    "\n",
    "results = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "    for threshold in [0.5, 0.75, 0.9, 0.99]:\n",
    "        pred, true = run_inference(\n",
    "            wide_mlp, df, threshold=threshold, batch_size=BATCH_SIZE\n",
    "        )\n",
    "        accuracy = accuracy_score(true, pred)\n",
    "        precision = precision_score(\n",
    "            true, pred, zero_division=0\n",
    "        )  # Handle potential division by zero\n",
    "        recall = recall_score(\n",
    "            true, pred, zero_division=0\n",
    "        )  # Handle potential division by zero\n",
    "        f1 = f1_score(true, pred, zero_division=0)  # Handle potential division by zero\n",
    "        cm = confusion_matrix(true, pred)\n",
    "        true_negatives, false_positives, false_negatives, true_positives = (\n",
    "            cm.ravel()\n",
    "        )  # Unpack confusion matrix into TN, FP, FN, TP\n",
    "        results[f\"{dataset_name}_{threshold}\"] = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"threshold\": threshold,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"true_negatives\": true_negatives,\n",
    "        }\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "results_df.to_csv(\"inference_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9095264d",
   "metadata": {},
   "source": [
    "# MLP 4cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d925de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = torch.load(\"widemlp-4cls_idf.pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bde8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 4-class PyTorch model on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load 4-class model\n",
    "checkpoint_4cls = torch.load(\n",
    "    \"widemlp-4cls.pt\", weights_only=True, map_location=torch.device(DEVICE)\n",
    ")\n",
    "wide_mlp_4cls = MLP(\n",
    "    vocab_size=len(tokenizer),\n",
    "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "    num_classes=4,  # Changed to 4 classes\n",
    "    idf=idf,\n",
    "    problem_type=\"multi_label_classification\",  # Changed to regular classification\n",
    ")\n",
    "wide_mlp_4cls.to(DEVICE)\n",
    "wide_mlp_4cls.idf = idf if wide_mlp_4cls.idf is not None else None\n",
    "wide_mlp_4cls.load_state_dict(checkpoint_4cls[\"model_state_dict\"])\n",
    "wide_mlp_4cls.eval()\n",
    "print(f\"Successfully loaded 4-class PyTorch model on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8358b59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing jigsaw dataset for 4-class model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/51 [00:00<?, ?it/s]c:\\Users\\risko\\Desktop\\ood-widemlp\\widemlp.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat_inputs = torch.cat([torch.tensor(doc) for doc in input_ids])\n",
      "Inference: 100%|██████████| 51/51 [00:02<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing olid dataset for 4-class model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/206 [00:00<?, ?it/s]c:\\Users\\risko\\Desktop\\ood-widemlp\\widemlp.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat_inputs = torch.cat([torch.tensor(doc) for doc in input_ids])\n",
      "Inference: 100%|██████████| 206/206 [00:02<00:00, 98.48it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing hate_xplain dataset for 4-class model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/93 [00:00<?, ?it/s]c:\\Users\\risko\\Desktop\\ood-widemlp\\widemlp.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat_inputs = torch.cat([torch.tensor(doc) for doc in input_ids])\n",
      "Inference: 100%|██████████| 93/93 [00:01<00:00, 75.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing tuke_sk dataset for 4-class model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/135 [00:00<?, ?it/s]c:\\Users\\risko\\Desktop\\ood-widemlp\\widemlp.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat_inputs = torch.cat([torch.tensor(doc) for doc in input_ids])\n",
      "Inference: 100%|██████████| 135/135 [00:02<00:00, 50.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dkk dataset for 4-class model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/1 [00:00<?, ?it/s]c:\\Users\\risko\\Desktop\\ood-widemlp\\widemlp.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat_inputs = torch.cat([torch.tensor(doc) for doc in input_ids])\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dkk_all dataset for 4-class model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference:   0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\risko\\Desktop\\ood-widemlp\\widemlp.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  flat_inputs = torch.cat([torch.tensor(doc) for doc in input_ids])\n",
      "Inference: 100%|██████████| 6/6 [00:00<00:00, 46.24it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_inference_4cls(\n",
    "    model: MLP,\n",
    "    dataset_df: pd.DataFrame,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    ") -> tuple:\n",
    "    dataset = Dataset.from_pandas(dataset_df)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            inputs = tokenizer(\n",
    "                batch[\"prompt\"],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(DEVICE)\n",
    "            flat_inputs, offsets = prepare_inputs_optimized(\n",
    "                inputs[\"input_ids\"], device=DEVICE\n",
    "            )\n",
    "            logits = model(flat_inputs, offsets)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            predictions = torch.argmax(probabilities, dim=1)\n",
    "            pred.extend(predictions.cpu().tolist())\n",
    "\n",
    "    return pred, dataset_df[\"label\"].tolist()\n",
    "\n",
    "# Run evaluation for 4-class model\n",
    "results_4cls = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {dataset_name} dataset for 4-class model...\")\n",
    "    pred, true = run_inference_4cls(\n",
    "        wide_mlp_4cls, df, batch_size=BATCH_SIZE\n",
    "    )\n",
    "    \n",
    "    # Convert predictions to binary (3 = harmful, 0,1,2 = non-harmful)\n",
    "    binary_pred = [0 if p == 3 else 1 for p in pred]\n",
    "    \n",
    "    accuracy = accuracy_score(true, binary_pred)\n",
    "    precision = precision_score(true, binary_pred, zero_division=0)\n",
    "    recall = recall_score(true, binary_pred, zero_division=0)\n",
    "    f1 = f1_score(true, binary_pred, zero_division=0)\n",
    "    cm = confusion_matrix(true, binary_pred)\n",
    "    \n",
    "    # Handle different confusion matrix shapes\n",
    "    cm_flat = cm.ravel()\n",
    "    if len(cm_flat) == 1:\n",
    "        # Only one class present\n",
    "        true_negatives = cm_flat[0]\n",
    "        false_positives = 0\n",
    "        false_negatives = 0\n",
    "        true_positives = 0\n",
    "    elif len(cm_flat) == 4:\n",
    "        true_negatives, false_positives, false_negatives, true_positives = cm_flat\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected confusion matrix shape for {dataset_name}\")\n",
    "        true_negatives = false_positives = false_negatives = true_positives = 0\n",
    "\n",
    "    results_4cls[dataset_name] = {\n",
    "        \"dataset_name\": dataset_name,\n",
    "        \"model_type\": \"4cls\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"true_positives\": true_positives,\n",
    "        \"false_positives\": false_positives,\n",
    "        \"false_negatives\": false_negatives,\n",
    "        \"true_negatives\": true_negatives,\n",
    "    }\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df_4cls = pd.DataFrame.from_dict(results_4cls, orient=\"index\")\n",
    "results_df_4cls.to_csv(\"inference_results_4cls.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda8e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
