{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49f7655",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30e250e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\risko\\miniconda3\\envs\\general\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb not installed, skipping wandb logging\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, concatenate_datasets, load_dataset\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    USE_WANDB = True\n",
    "except ImportError:\n",
    "    USE_WANDB = False\n",
    "    print(\"wandb not installed, skipping wandb logging\")\n",
    "from widemlp import MLP, inverse_document_frequency, prepare_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad122a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "DEVICE = (\n",
    "    torch.device(\"cuda:0\")\n",
    "    if torch.cuda.is_available()\n",
    "    else torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    ")\n",
    "BATCH_SIZE = 64\n",
    "SEED = 42\n",
    "MODEL_NAME = \"\"\n",
    "EPOCHS = 1\n",
    "NUM_CLASSES = 3\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "TRAIN_LOGGING_STEPS = 1\n",
    "EVAL_LOGGING_STEPS = 250\n",
    "THRESHOLD = 0.5\n",
    "DATASET_SIZE = 15_000\n",
    "LOG_WANDB = False\n",
    "PATH = \"widemlp.pt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e8e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA devices:\n",
      "  0: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "# find cuda devices\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA devices:\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731d5d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOG_WANDB:\n",
    "    wandb.init(project=\"ood-widemlp\")\n",
    "    wandb.config.update(\n",
    "        {\n",
    "            \"seed\": SEED,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"epochs\": EPOCHS,\n",
    "            \"gradient_accumulation_steps\": GRADIENT_ACCUMULATION_STEPS,\n",
    "            \"train_logging_steps\": TRAIN_LOGGING_STEPS,\n",
    "            \"eval_logging_steps\": EVAL_LOGGING_STEPS,\n",
    "            \"threshold\": THRESHOLD,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ec5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def load_data(path: str, eval_size: int = 0.2) -> Dataset:\n",
    "    df = pd.read_json(path) if path.endswith(\".json\") else pd.read_csv(path)\n",
    "    dataset = Dataset.from_pandas(df)\n",
    "    dataset.shuffle(seed=42)\n",
    "    split_dataset = dataset.train_test_split(test_size=eval_size)\n",
    "    return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c87e07e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d83e6d2",
   "metadata": {},
   "source": [
    "# Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4644cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "law_dataset = load_dataset(\"dim/law_stackexchange_prompts\")\n",
    "finance_dataset = load_dataset(\"4DR1455/finance_questions\")\n",
    "healthcare_dataset = load_dataset(\"iecjsu/lavita-ChatDoctor-HealthCareMagic-100k\")\n",
    "\n",
    "keep = [\"text\", \"domain\", \"label\"]\n",
    "\n",
    "# Filter and prepare law dataset\n",
    "law_data = (\n",
    "    law_dataset[\"train\"]\n",
    "    .filter(lambda x: x[\"prompt\"] is not None and x[\"prompt\"].strip() != \"\")\n",
    "    .filter(lambda x: all(v is not None for v in x.values()))\n",
    "    .select(range(min(DATASET_SIZE, len(law_dataset[\"train\"]))))\n",
    "    .map(\n",
    "        lambda x: {\"text\": x[\"prompt\"], \"domain\": \"law\", \"label\": 0},\n",
    "        remove_columns=[c for c in law_dataset[\"train\"].column_names if c not in keep],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter and prepare finance dataset\n",
    "finance_data = (\n",
    "    finance_dataset[\"train\"]\n",
    "    .filter(\n",
    "        lambda x: x[\"instruction\"] is not None\n",
    "        and len(str(x[\"instruction\"]).strip()) > 0\n",
    "    )\n",
    "    .filter(lambda x: all(v is not None for v in x.values()))\n",
    "    .select(range(min(DATASET_SIZE, len(finance_dataset[\"train\"]))))\n",
    "    .map(\n",
    "        lambda x: {\"text\": str(x[\"instruction\"]), \"domain\": \"finance\", \"label\": 1},\n",
    "        remove_columns=[\n",
    "            c for c in finance_dataset[\"train\"].column_names if c not in keep\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# Filter and prepare healthcare dataset\n",
    "healthcare_data = (\n",
    "    healthcare_dataset[\"train\"]\n",
    "    .filter(lambda x: x[\"input\"] is not None and len(str(x[\"input\"]).strip()) > 0)\n",
    "    .filter(lambda x: all(v is not None for v in x.values()))\n",
    "    .select(range(min(DATASET_SIZE, len(healthcare_dataset[\"train\"]))))\n",
    "    .map(\n",
    "        lambda x: {\"text\": str(x[\"input\"]), \"domain\": \"healthcare\", \"label\": 2},\n",
    "        remove_columns=[\n",
    "            c for c in healthcare_dataset[\"train\"].column_names if c not in keep\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Concatenate datasets\n",
    "combined_dataset = concatenate_datasets([law_data, finance_data, healthcare_data])\n",
    "\n",
    "# Split into train and test sets using dataset's train_test_split method\n",
    "data = combined_dataset.train_test_split(test_size=0.2, seed=SEED)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    data[\"train\"], batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    data[\"test\"], batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1f4cb",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9acbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing IDF: 36000it [01:41, 353.04it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = [\n",
    "    tokenizer.encode(raw_doc, padding=False, truncation=True, max_length=None)\n",
    "    for raw_doc in data[\"train\"][\"text\"]\n",
    "]\n",
    "idf = inverse_document_frequency(docs, len(tokenizer))\n",
    "model = MLP(\n",
    "    vocab_size=len(tokenizer),\n",
    "    num_hidden_layers=3,\n",
    "    num_classes=3,\n",
    "    idf=idf,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "model.to(DEVICE)\n",
    "model.idf = model.idf.to(DEVICE) if model.idf is not None else None\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model: torch.nn.Module,\n",
    "    valid_loader: torch.utils.data.DataLoader,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    threshold: float,\n",
    ") -> dict:\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    validation_losses = []\n",
    "    max_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            inputs = tokenizer(\n",
    "                batch[\"text\"],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(DEVICE)\n",
    "            flat_inputs, offsets = prepare_inputs(inputs[\"input_ids\"], device=DEVICE)\n",
    "            labels = batch[\"label\"].to(DEVICE, dtype=torch.long)\n",
    "            one_hot_targets = torch.nn.functional.one_hot(\n",
    "                labels, num_classes=NUM_CLASSES\n",
    "            ).to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "            loss, logits = model(flat_inputs, offsets, one_hot_targets)\n",
    "            validation_losses.append(loss.item())\n",
    "            probabilities = torch.sigmoid(logits)  # -> 0-1\n",
    "            batch_predictions = []\n",
    "            for i in range(probabilities.size(0)):\n",
    "                sample_probabilities = probabilities[i]\n",
    "                max_probs.append(sample_probabilities.max().item())\n",
    "                thresholded_labels_indices = torch.where(\n",
    "                    sample_probabilities > threshold\n",
    "                )[0]\n",
    "                if len(thresholded_labels_indices) > 1:\n",
    "                    best_label_index = thresholded_labels_indices[\n",
    "                        torch.argmax(sample_probabilities[thresholded_labels_indices])\n",
    "                    ]\n",
    "                    prediction_vector = torch.zeros(NUM_CLASSES, dtype=torch.int)\n",
    "                    prediction_vector[best_label_index] = 1\n",
    "                    batch_predictions.append(prediction_vector.cpu().numpy())\n",
    "                else:\n",
    "                    predictions = (sample_probabilities > threshold).int()\n",
    "                    batch_predictions.append(predictions.cpu().numpy())\n",
    "            all_predictions.extend(batch_predictions)\n",
    "            all_labels.extend(one_hot_targets.cpu().numpy())\n",
    "\n",
    "    avg_validation_loss = np.mean(validation_losses)\n",
    "\n",
    "    # Convert to numpy arrays for metric calculation\n",
    "    all_predictions_np = np.array(all_predictions)\n",
    "    all_labels_np = np.array(all_labels)\n",
    "\n",
    "    # Calculate metrics\n",
    "    macro_f1 = f1_score(\n",
    "        all_labels_np, all_predictions_np, average=\"macro\", zero_division=0\n",
    "    )  # zero_division=0 to handle cases with no predicted labels\n",
    "    micro_f1 = f1_score(\n",
    "        all_labels_np, all_predictions_np, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    macro_precision = precision_score(\n",
    "        all_labels_np, all_predictions_np, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    macro_recall = recall_score(\n",
    "        all_labels_np, all_predictions_np, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    micro_precision = precision_score(\n",
    "        all_labels_np, all_predictions_np, average=\"micro\", zero_division=0\n",
    "    )\n",
    "    micro_recall = recall_score(\n",
    "        all_labels_np, all_predictions_np, average=\"micro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    scores = {\n",
    "        \"validation_loss\": avg_validation_loss,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"macro_precision\": macro_precision,\n",
    "        \"macro_recall\": macro_recall,\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"max_probs\": np.mean(max_probs),\n",
    "    }\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c835176",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "for _epoch in range(EPOCHS):\n",
    "    step = 0\n",
    "    batch_train_loss = []\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        model.train()\n",
    "        inputs = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=False,\n",
    "        )\n",
    "        flat_inputs, offsets = prepare_inputs(inputs[\"input_ids\"], device=DEVICE)\n",
    "        labels = batch[\"label\"].to(DEVICE, dtype=torch.long)\n",
    "        one_hot_targets = torch.nn.functional.one_hot(\n",
    "            labels, num_classes=NUM_CLASSES\n",
    "        ).to(DEVICE, dtype=torch.float32)\n",
    "        # inputs[\"label\"] = one_hot_targets\n",
    "        loss, logits = model(flat_inputs, offsets, one_hot_targets)\n",
    "        loss.backward()\n",
    "        step += 1  # noqa: SIM113\n",
    "        batch_train_loss.append(loss.item())\n",
    "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if step % TRAIN_LOGGING_STEPS == 0:\n",
    "            accuracy = np.mean(batch_train_loss)\n",
    "            if LOG_WANDB:\n",
    "                wandb.log({\"train_loss\": float(accuracy)})\n",
    "            else:\n",
    "                print(f\"Step {step}, Train Loss: {accuracy:.4f}\")\n",
    "        if step % EVAL_LOGGING_STEPS == 0:\n",
    "            scores = evaluate(model, valid_loader, tokenizer, threshold=THRESHOLD)\n",
    "            if LOG_WANDB:\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"validation_loss\": float(scores[\"validation_loss\"]),\n",
    "                        \"test/macro_f1\": float(scores[\"macro_f1\"]),\n",
    "                        \"test/micro_f1\": float(scores[\"micro_f1\"]),\n",
    "                        \"test/macro_recall\": float(scores[\"macro_recall\"]),\n",
    "                        \"test/micro_recall\": float(scores[\"micro_recall\"]),\n",
    "                        \"test/macro_precision\": float(scores[\"macro_precision\"]),\n",
    "                        \"test/micro_precision\": float(scores[\"micro_precision\"]),\n",
    "                        \"test/max_probs\": float(scores[\"max_probs\"]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Step {step}, Validation Loss: {scores['validation_loss']:.4f}, Macro F1: {scores['macro_f1']:.4f}, Micro F1: {scores['micro_f1']:.4f}\"\n",
    "                )\n",
    "if LOG_WANDB:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(\n",
    "    {\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    \"widemlp.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c245e3",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ffda7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded PyTorch model on cuda:0\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "checkpoint = torch.load(\"widemlp.pt\", weights_only=True)\n",
    "wide_mlp = MLP(\n",
    "    vocab_size=len(tokenizer),\n",
    "    num_hidden_layers=3,\n",
    "    num_classes=3,\n",
    "    idf=idf,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "wide_mlp.to(DEVICE)\n",
    "wide_mlp.idf = model.idf.to(DEVICE) if wide_mlp.idf is not None else None\n",
    "wide_mlp.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "wide_mlp.eval()\n",
    "print(f\"Successfully loaded PyTorch model on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e67aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Jigsaw dataset\n",
    "jigsaw_splits = {\n",
    "    \"train\": \"train_dataset.csv\",\n",
    "    \"validation\": \"val_dataset.csv\",\n",
    "    \"test\": \"test_dataset.csv\",\n",
    "}\n",
    "jigsaw_df = pd.read_csv(\n",
    "    \"hf://datasets/Arsive/toxicity_classification_jigsaw/\" + jigsaw_splits[\"validation\"]\n",
    ")\n",
    "\n",
    "jigsaw_df = jigsaw_df[\n",
    "    (jigsaw_df[\"toxic\"] == 1)\n",
    "    | (jigsaw_df[\"severe_toxic\"] == 1)\n",
    "    | (jigsaw_df[\"obscene\"] == 1)\n",
    "    | (jigsaw_df[\"threat\"] == 1)\n",
    "    | (jigsaw_df[\"insult\"] == 1)\n",
    "    | (jigsaw_df[\"identity_hate\"] == 1)\n",
    "]\n",
    "\n",
    "jigsaw_df = jigsaw_df.rename(columns={\"comment_text\": \"prompt\"})\n",
    "jigsaw_df[\"label\"] = 0\n",
    "jigsaw_df = jigsaw_df[[\"prompt\", \"label\"]]\n",
    "jigsaw_df = jigsaw_df.dropna(subset=[\"prompt\"])\n",
    "jigsaw_df = jigsaw_df[jigsaw_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load OLID dataset\n",
    "olid_splits = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
    "olid_df = pd.read_csv(\"hf://datasets/christophsonntag/OLID/\" + olid_splits[\"train\"])\n",
    "olid_df = olid_df.rename(columns={\"cleaned_tweet\": \"prompt\"})\n",
    "olid_df[\"label\"] = 0\n",
    "olid_df = olid_df[[\"prompt\", \"label\"]]\n",
    "olid_df = olid_df.dropna(subset=[\"prompt\"])\n",
    "olid_df = olid_df[olid_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load hateXplain dataset\n",
    "hateXplain = pd.read_parquet(\n",
    "    \"hf://datasets/nirmalendu01/hateXplain_filtered/data/train-00000-of-00001.parquet\"\n",
    ")\n",
    "hateXplain = hateXplain.rename(columns={\"test_case\": \"prompt\"})\n",
    "hateXplain = hateXplain[(hateXplain[\"gold_label\"] == \"hateful\")]\n",
    "hateXplain = hateXplain[[\"prompt\", \"label\"]]\n",
    "hateXplain[\"label\"] = 0\n",
    "hateXplain = hateXplain.dropna(subset=[\"prompt\"])\n",
    "hateXplain = hateXplain[hateXplain[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load TUKE Slovak dataset\n",
    "tuke_sk_splits = {\"train\": \"train.json\", \"test\": \"test.json\"}\n",
    "tuke_sk_df = pd.read_json(\n",
    "    \"hf://datasets/TUKE-KEMT/hate_speech_slovak/\" + tuke_sk_splits[\"train\"], lines=True\n",
    ")\n",
    "tuke_sk_df = tuke_sk_df.rename(columns={\"text\": \"prompt\"})\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"label\"] == 0]\n",
    "tuke_sk_df = tuke_sk_df[[\"prompt\", \"label\"]]\n",
    "tuke_sk_df = tuke_sk_df.dropna(subset=[\"prompt\"])\n",
    "tuke_sk_df = tuke_sk_df[tuke_sk_df[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "# Load DKK dataset\n",
    "dkk = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "dkk = dkk.rename(columns={\"text\": \"prompt\"})\n",
    "dkk = dkk[dkk[\"label\"] == \"OFF\"].reset_index(drop=True)\n",
    "dkk[\"label\"] = 0\n",
    "dkk = dkk.dropna(subset=[\"prompt\"])\n",
    "dkk = dkk[dkk[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "dkk_all = pd.read_parquet(\"data/test-00000-of-00001.parquet\")\n",
    "dkk_all = dkk_all.rename(columns={\"text\": \"prompt\"})\n",
    "dkk_all[\"label\"] = 0\n",
    "dkk_all = dkk_all.dropna(subset=[\"prompt\"])\n",
    "dkk_all = dkk_all[dkk_all[\"prompt\"].str.strip() != \"\"]\n",
    "\n",
    "datasets = {\n",
    "    \"jigsaw\": jigsaw_df,\n",
    "    \"olid\": olid_df,\n",
    "    \"hate_xplain\": hateXplain,\n",
    "    \"tuke_sk\": tuke_sk_df,\n",
    "    \"dkk\": dkk,\n",
    "    \"dkk_all\": dkk_all,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52b1b20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing jigsaw dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 51/51 [01:41<00:00,  1.99s/it]\n",
      "Inference: 100%|██████████| 51/51 [01:39<00:00,  1.96s/it]\n",
      "Inference: 100%|██████████| 51/51 [01:42<00:00,  2.02s/it]\n",
      "Inference: 100%|██████████| 51/51 [01:43<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing olid dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 206/206 [00:18<00:00, 11.00it/s]\n",
      "Inference: 100%|██████████| 206/206 [00:18<00:00, 11.06it/s]\n",
      "Inference: 100%|██████████| 206/206 [00:19<00:00, 10.39it/s]\n",
      "Inference: 100%|██████████| 206/206 [00:18<00:00, 11.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing hate_xplain dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 93/93 [00:14<00:00,  6.56it/s]\n",
      "Inference: 100%|██████████| 93/93 [00:13<00:00,  6.66it/s]\n",
      "Inference: 100%|██████████| 93/93 [00:14<00:00,  6.63it/s]\n",
      "Inference: 100%|██████████| 93/93 [00:14<00:00,  6.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing tuke_sk dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 135/135 [01:03<00:00,  2.11it/s]\n",
      "Inference: 100%|██████████| 135/135 [01:03<00:00,  2.12it/s]\n",
      "Inference: 100%|██████████| 135/135 [01:03<00:00,  2.12it/s]\n",
      "Inference: 100%|██████████| 135/135 [01:04<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dkk dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s]\n",
      "Inference: 100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n",
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dkk_all dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 6/6 [00:02<00:00,  2.10it/s]\n",
      "Inference: 100%|██████████| 6/6 [00:03<00:00,  1.98it/s]\n",
      "Inference: 100%|██████████| 6/6 [00:02<00:00,  2.06it/s]\n",
      "Inference: 100%|██████████| 6/6 [00:02<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def run_inference(model, dataset_df, threshold=THRESHOLD):\n",
    "    dataset = Dataset.from_pandas(dataset_df)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    "    )\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\"):\n",
    "            inputs = tokenizer(\n",
    "                batch[\"prompt\"],\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).to(DEVICE)\n",
    "            flat_inputs, offsets = prepare_inputs(inputs[\"input_ids\"], device=DEVICE)\n",
    "\n",
    "            # Forward pass without labels for inference\n",
    "            logits = model(flat_inputs, offsets)\n",
    "            probabilities = torch.sigmoid(logits) # -> 0-1 mostly one class is around 0.99+ in classification \n",
    "\n",
    "            # Classification of hate speech based on thresholds of class probabilities\n",
    "            for probs in probabilities:\n",
    "                thresholded_indices = torch.where(probs > threshold)[0]\n",
    "                if len(thresholded_indices) == 0:\n",
    "                    pred.append(0)\n",
    "                else:\n",
    "                    pred.append(1)\n",
    "    return pred, dataset_df[\"label\"].tolist()\n",
    "\n",
    "# Run inference on each dataset\n",
    "results = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "    for threshold in [0.5, 0.75, 0.9, 0.99]:\n",
    "        pred, true = run_inference(wide_mlp, df, threshold)\n",
    "        \n",
    "        # Calculate metrics (swawp with scikit-learn)\n",
    "        true_positives = sum(1 for p, t in zip(pred, true) if p == 1 and t == 1)\n",
    "        false_positives = sum(1 for p, t in zip(pred, true) if p == 1 and t == 0)\n",
    "        false_negatives = sum(1 for p, t in zip(pred, true) if p == 0 and t == 1)\n",
    "        true_negatives = sum(1 for p, t in zip(pred, true) if p == 0 and t == 0)\n",
    "        \n",
    "        accuracy = (true_positives + true_negatives) / len(pred)\n",
    "        precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results[f\"{dataset_name}_{threshold}\"] = {\n",
    "            \"dataset_name\": dataset_name,\n",
    "            \"threshold\": threshold,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"true_positives\": true_positives,\n",
    "            \"false_positives\": false_positives,\n",
    "            \"false_negatives\": false_negatives,\n",
    "            \"true_negatives\": true_negatives\n",
    "        }\n",
    "\n",
    "# Convert results to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "results_df.to_csv('inference_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9aafc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
